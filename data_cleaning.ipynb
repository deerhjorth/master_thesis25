{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 1)) (2.2.2)\n",
      "Requirement already satisfied: numpy in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 3)) (3.8.4)\n",
      "Requirement already satisfied: requests in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 4)) (2.31.0)\n",
      "Requirement already satisfied: gdown in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 5)) (5.2.0)\n",
      "Requirement already satisfied: psycopg2 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 6)) (2.9.10)\n",
      "Requirement already satisfied: spacy in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 7)) (3.7.4)\n",
      "Requirement already satisfied: gensim in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 8)) (4.3.2)\n",
      "Requirement already satisfied: nltk in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 9)) (3.8.1)\n",
      "Requirement already satisfied: tqdm in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 10)) (4.66.4)\n",
      "Requirement already satisfied: bertopic in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 11)) (0.16.2)\n",
      "Requirement already satisfied: sentence-transformers in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 12)) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 13)) (1.4.2)\n",
      "Collecting vaderSentiment (from -r requirements.txt (line 14))\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from pandas->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from pandas->-r requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from pandas->-r requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from matplotlib->-r requirements.txt (line 3)) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from matplotlib->-r requirements.txt (line 3)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from matplotlib->-r requirements.txt (line 3)) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from matplotlib->-r requirements.txt (line 3)) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from matplotlib->-r requirements.txt (line 3)) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from matplotlib->-r requirements.txt (line 3)) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from matplotlib->-r requirements.txt (line 3)) (3.1.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from matplotlib->-r requirements.txt (line 3)) (6.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from requests->-r requirements.txt (line 4)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from requests->-r requirements.txt (line 4)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from requests->-r requirements.txt (line 4)) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from requests->-r requirements.txt (line 4)) (2024.2.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from gdown->-r requirements.txt (line 5)) (4.13.3)\n",
      "Requirement already satisfied: filelock in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from gdown->-r requirements.txt (line 5)) (3.14.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from spacy->-r requirements.txt (line 7)) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from spacy->-r requirements.txt (line 7)) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from spacy->-r requirements.txt (line 7)) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from spacy->-r requirements.txt (line 7)) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from spacy->-r requirements.txt (line 7)) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from spacy->-r requirements.txt (line 7)) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from spacy->-r requirements.txt (line 7)) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from spacy->-r requirements.txt (line 7)) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from spacy->-r requirements.txt (line 7)) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from spacy->-r requirements.txt (line 7)) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from spacy->-r requirements.txt (line 7)) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from spacy->-r requirements.txt (line 7)) (6.4.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from spacy->-r requirements.txt (line 7)) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from spacy->-r requirements.txt (line 7)) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from spacy->-r requirements.txt (line 7)) (58.0.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from spacy->-r requirements.txt (line 7)) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from gensim->-r requirements.txt (line 8)) (1.10.1)\n",
      "Requirement already satisfied: click in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from nltk->-r requirements.txt (line 9)) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from nltk->-r requirements.txt (line 9)) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from nltk->-r requirements.txt (line 9)) (2024.5.15)\n",
      "Requirement already satisfied: hdbscan>=0.8.29 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from bertopic->-r requirements.txt (line 11)) (0.8.33)\n",
      "Requirement already satisfied: umap-learn>=0.5.0 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from bertopic->-r requirements.txt (line 11)) (0.5.6)\n",
      "Requirement already satisfied: plotly>=4.7.0 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from bertopic->-r requirements.txt (line 11)) (5.22.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from sentence-transformers->-r requirements.txt (line 12)) (4.41.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from sentence-transformers->-r requirements.txt (line 12)) (2.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from sentence-transformers->-r requirements.txt (line 12)) (0.23.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from scikit-learn->-r requirements.txt (line 13)) (3.5.0)\n",
      "Requirement already satisfied: cython<3,>=0.27 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from hdbscan>=0.8.29->bertopic->-r requirements.txt (line 11)) (0.29.37)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.15.1->sentence-transformers->-r requirements.txt (line 12)) (2024.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.15.1->sentence-transformers->-r requirements.txt (line 12)) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.15.1->sentence-transformers->-r requirements.txt (line 12)) (4.11.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib->-r requirements.txt (line 3)) (3.18.1)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from langcodes<4.0.0,>=3.2.0->spacy->-r requirements.txt (line 7)) (1.2.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from plotly>=4.7.0->bertopic->-r requirements.txt (line 11)) (8.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 7)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 7)) (2.18.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 1)) (1.15.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from thinc<8.3.0,>=8.2.2->spacy->-r requirements.txt (line 7)) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from thinc<8.3.0,>=8.2.2->spacy->-r requirements.txt (line 7)) (0.1.4)\n",
      "Requirement already satisfied: sympy in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 12)) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 12)) (3.2.1)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers->-r requirements.txt (line 12)) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers->-r requirements.txt (line 12)) (0.4.3)\n",
      "Requirement already satisfied: numba>=0.51.2 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from umap-learn>=0.5.0->bertopic->-r requirements.txt (line 11)) (0.59.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from umap-learn>=0.5.0->bertopic->-r requirements.txt (line 11)) (0.5.12)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from weasel<0.4.0,>=0.1.0->spacy->-r requirements.txt (line 7)) (0.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from beautifulsoup4->gdown->-r requirements.txt (line 5)) (2.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from jinja2->spacy->-r requirements.txt (line 7)) (2.1.5)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (1.7.1)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->-r requirements.txt (line 7)) (1.1.1)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic->-r requirements.txt (line 11)) (0.42.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from sympy->torch>=1.11.0->sentence-transformers->-r requirements.txt (line 12)) (1.3.0)\n",
      "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# If necessary, install packages\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gdown in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from gdown) (4.13.3)\n",
      "Requirement already satisfied: filelock in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from gdown) (3.14.0)\n",
      "Requirement already satisfied: requests[socks] in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from gdown) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from gdown) (4.66.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from beautifulsoup4->gdown) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from beautifulsoup4->gdown) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from requests[socks]->gdown) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from requests[socks]->gdown) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from requests[socks]->gdown) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from requests[socks]->gdown) (2024.2.2)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file IDs and table namesfor lobbying data files from Google Drive\n",
    "lob_rpt = (\"1CIq7XwaFTJmnUKdAvYyh53Et2XSJRX9d\", \"lob_rpt\")\n",
    "lob_lobbyist = (\"1q1ZyLvUhsxsbPLdi_CmrNH0FqLoorr6p\", \"lob_lobbyist\")\n",
    "lob_lobbying = (\"19Jef89spXbkExNpYA-s_e8GRii-0tReu\", \"lob_lobbying\")\n",
    "lob_issue = (\"18-D_vW4dAHJlvFs4ARK4lgx6KdsAXWoq\", \"lob_issue\")\n",
    "lob_indus = (\"1yldZYLcZTLIMQ5Xa-e-ecGgQGFWv5n4A\", \"lob_indus\")\n",
    "lob_bills = (\"1qqQvNXtdhyI8KYcctpVUZ3FoIwvjWaG1\", \"lob_bills\")\n",
    "lob_agency = (\"1CFi0Itpi_qg4X4wicg1FIg8S_FJfvCN0\", \"lob_agency\")\n",
    "CatCodeReference = (\"1vfVA-v-lUMuykF9efl2oDN5DJK2t1PwA\", \"CatCodeReference\")\n",
    "CandidateIds = (\"1k2lhAcuSQbn5z7NN3USPEbX_Rg5VV_Ky\", \"CandidateIds\")\n",
    "congress_bill_data_id = (\"1-MnzV0iZ2FPYiuQgOmggjmXvxGmBzYV8\", \"congress_bill_data\")\n",
    "tweet_data = ('1Cm0hVghQf47Ep7WyP8Rx4vusSFJ9t47T','tweets_cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gdown\n",
    "import os\n",
    "\n",
    "def load_csv(table, save_file=False, file_path=None, gzip=False):\n",
    "    \"\"\"\n",
    "    Downloads a large CSV file from Google Drive into a Pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        file_info (tuple): A tuple containing the Google Drive file ID and the table name.\n",
    "        save_file (bool): If True, the file is saved. If False, it is deleted after loading.\n",
    "        file_path (str, optional): The directory path where the file should be saved. Defaults to current directory.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The loaded Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    file_id, table_name = table\n",
    "    \n",
    "    # Set filename as the table name\n",
    "    temp_file = f\"{table_name}.csv\"\n",
    "    \n",
    "    # If a file path is provided, join it with the filename\n",
    "    if file_path:\n",
    "        temp_file = os.path.join(file_path, temp_file)\n",
    "\n",
    "    # Construct Google Drive download URL\n",
    "    url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
    "\n",
    "    # Download the file\n",
    "    gdown.download(url, temp_file, quiet=False)\n",
    "\n",
    "    # Load the CSV file into a Pandas DataFrame\n",
    "    if gzip == False:\n",
    "        df = pd.read_csv(temp_file,header=None)\n",
    "    else:\n",
    "        df = pd.read_csv(temp_file,header=None, compression=\"gzip\")\n",
    "\n",
    "    # Remove the file if save_file is False\n",
    "    if not save_file:\n",
    "        os.remove(temp_file)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?export=download&id=1CIq7XwaFTJmnUKdAvYyh53Et2XSJRX9d\n",
      "To: /Users/kristianmadslangrud/Documents/GitHub/master_thesis25/lob_rpt.csv\n",
      "100%|██████████| 3.29k/3.29k [00:00<00:00, 3.08MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  0    1\n",
      "0                   MID-YEAR REPORT    m\n",
      "1                MID-YEAR AMENDMENT   ma\n",
      "2            MID-YEAR (NO ACTIVITY)   mn\n",
      "3  MID-YEAR AMENDMENT (NO ACTIVITY)  man\n",
      "4              MID-YEAR TERMINATION   mt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "df = load_csv(lob_rpt, save_file=False)  # Specify file_path if needed\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating function to visualize key aspects of each dataframe. \n",
    "def glimpse(df):\n",
    "    print(f'\\n Shape: {df.shape}')\n",
    "    print(f'\\n Columns: {list(df.columns)}')\n",
    "    print(f'\\n Data types:\\n{df.dtypes}')\n",
    "    \n",
    "    missing = df.isnull().sum()\n",
    "    missing_percent = (missing / len(df)) * 100\n",
    "    \n",
    "    print(f'\\n Missing values:\\n{missing[missing > 0]}')\n",
    "    print(f'\\n Missing values (%):\\n{missing_percent[missing_percent > 0].round(2)}')\n",
    "    \n",
    "    print(f'\\n First 5 rows:\\n{df.head()}')\n",
    "    print(f'\\n Last 5 rows:\\n{df.tail()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lobbying Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?export=download&id=19Jef89spXbkExNpYA-s_e8GRii-0tReu\n",
      "From (redirected): https://drive.google.com/uc?export=download&id=19Jef89spXbkExNpYA-s_e8GRii-0tReu&confirm=t&uuid=c76a9f2c-bcb6-4642-89fa-05bfcc07f93b\n",
      "To: /Users/kristianmadslangrud/Documents/GitHub/master_thesis25/lob_lobbying.csv\n",
      "100%|██████████| 351M/351M [00:07<00:00, 48.3MB/s] \n",
      "/var/folders/pg/ghgx2xgn3qb65km4qhtfgzhr0000gn/T/ipykernel_58531/2418707545.py:33: DtypeWarning: Columns (7,11,14,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(temp_file,header=None)\n"
     ]
    }
   ],
   "source": [
    "df_lobbying = load_csv(lob_lobbying, save_file = False)\n",
    "df_lobbying.columns = ['UniqID','Registrant_Raw','Registrant','IsLobbyingFirm','Client_raw','Client','UItorg','Amount','Catcode','Source','Self','IncludeNSFS','Use','Ind','Year','Type','Typelong','Affiliate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping unecessary columns\n",
    "df_lobbying = df_lobbying.drop(\n",
    "['Registrant_Raw', #Same as Registrant\n",
    " 'Client_raw', # Same as Client\n",
    " 'Source', # Irrelevant to analysis\n",
    " 'Typelong' # Same as Type\n",
    "], axis=1)\n",
    "\n",
    "# Removing all records from lobbying dataframe without an ID\n",
    "df_lobbying = df_lobbying.dropna(subset=['UniqID'])\n",
    "\n",
    "# Capitalizing all inputs in Catcode column\n",
    "df_lobbying['Catcode'] = df_lobbying['Catcode'].str.upper()\n",
    "\n",
    "# Replacing missing values in selected boolean columns with 'n' column with 'n'\n",
    "boolean_columns = ['IsLobbyingFirm','Affiliate','IncludeNSFS','Ind']\n",
    "df_lobbying[boolean_columns] = df_lobbying[boolean_columns].fillna(value='n')\n",
    "\n",
    "# Changing datatype of columns\n",
    "df_lobbying['Amount'] = pd.to_numeric(df_lobbying['Amount'], errors='coerce')\n",
    "df_lobbying = df_lobbying.dropna(subset=['Amount'])\n",
    "df_lobbying['Year'] = df_lobbying['Year'].astype('int')\n",
    "df_lobbying[boolean_columns] = df_lobbying[boolean_columns].apply(lambda col: col.map({'y': True, 'n': False}).astype(bool))\n",
    "\n",
    "# Dropping all lobbying records with Use == n, as these reports have been amendent/updated and a more recent report is avaliable.  \n",
    "df_lobbying = df_lobbying[df_lobbying['Use'] != 'n']\n",
    "df_lobbying = df_lobbying.drop('Use',axis=1)\n",
    "\n",
    "# Only including records after 2011.\n",
    "df_lobbying = df_lobbying[df_lobbying['Year'] >= 2011]\n",
    "df_lobbying = df_lobbying.drop('Affiliate',axis=1) # Dropping the Affiliate column as all records are equal to False after Slicing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UniqID</th>\n",
       "      <th>Registrant</th>\n",
       "      <th>IsLobbyingFirm</th>\n",
       "      <th>Client</th>\n",
       "      <th>UItorg</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Catcode</th>\n",
       "      <th>Self</th>\n",
       "      <th>IncludeNSFS</th>\n",
       "      <th>Ind</th>\n",
       "      <th>Year</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>ReportType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>82c5f661-a637-45ad-a3a6-b5ba18cf8962</td>\n",
       "      <td>AstraZeneca Pharmaceuticals</td>\n",
       "      <td>False</td>\n",
       "      <td>AstraZeneca Pharmaceuticals</td>\n",
       "      <td>AstraZeneca PLC</td>\n",
       "      <td>1370000.0</td>\n",
       "      <td>H4300</td>\n",
       "      <td>x</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2021</td>\n",
       "      <td>Q4</td>\n",
       "      <td>Amendment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84ad3a9e-5864-4227-a802-e268fbf37237</td>\n",
       "      <td>David L Horne LLC</td>\n",
       "      <td>True</td>\n",
       "      <td>Multifamily Lenders Council</td>\n",
       "      <td>Multifamily Lenders Council</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>F4600</td>\n",
       "      <td>n</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2021</td>\n",
       "      <td>Q4</td>\n",
       "      <td>Standard Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85b111b1-5d2e-4107-bc24-0921316e29a5</td>\n",
       "      <td>Echelon Government Affairs</td>\n",
       "      <td>True</td>\n",
       "      <td>Albers Group</td>\n",
       "      <td>Albers Group</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>Y4000</td>\n",
       "      <td>n</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2021</td>\n",
       "      <td>Q4</td>\n",
       "      <td>Standard Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87822a14-12de-478c-a34d-010fa503e539</td>\n",
       "      <td>Western Telecommunications Alliance</td>\n",
       "      <td>False</td>\n",
       "      <td>Western Telecommunications Alliance</td>\n",
       "      <td>Western Telecommunications Alliance</td>\n",
       "      <td>75000.0</td>\n",
       "      <td>C4000</td>\n",
       "      <td>p</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2021</td>\n",
       "      <td>Q4</td>\n",
       "      <td>Standard Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87ff989d-9d12-4fef-84ef-ab69cd616894</td>\n",
       "      <td>Financial Executives International</td>\n",
       "      <td>False</td>\n",
       "      <td>Financial Executives International</td>\n",
       "      <td>Financial Executives International</td>\n",
       "      <td>21650.0</td>\n",
       "      <td>F5000</td>\n",
       "      <td>p</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2021</td>\n",
       "      <td>Q4</td>\n",
       "      <td>Standard Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544068</th>\n",
       "      <td>0854dd96-ed91-4219-99d9-b84bf06ee2f8</td>\n",
       "      <td>American Express</td>\n",
       "      <td>False</td>\n",
       "      <td>American Express</td>\n",
       "      <td>American Express</td>\n",
       "      <td>440000.0</td>\n",
       "      <td>F1400</td>\n",
       "      <td>p</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2022</td>\n",
       "      <td>Q4</td>\n",
       "      <td>Standard Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544069</th>\n",
       "      <td>086b65ba-0abe-4ed0-a3e9-a66c9ecc96a3</td>\n",
       "      <td>BL Partners Group</td>\n",
       "      <td>True</td>\n",
       "      <td>CTIA</td>\n",
       "      <td>CTIA</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>C4300</td>\n",
       "      <td>i</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2022</td>\n",
       "      <td>Q4</td>\n",
       "      <td>Standard Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544070</th>\n",
       "      <td>b01b51ba-d426-4750-ad00-0ea3c5aa2317</td>\n",
       "      <td>Vectis DC</td>\n",
       "      <td>True</td>\n",
       "      <td>City of Montebello, CA</td>\n",
       "      <td>City of Montebello, CA</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>X3000</td>\n",
       "      <td>n</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2022</td>\n",
       "      <td>Q4</td>\n",
       "      <td>Standard Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544071</th>\n",
       "      <td>b0235c16-0a34-4e06-be62-f42a3bf52d6a</td>\n",
       "      <td>Nickles Group</td>\n",
       "      <td>True</td>\n",
       "      <td>Walmart Inc</td>\n",
       "      <td>Walmart Inc</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>G4300</td>\n",
       "      <td>i</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2022</td>\n",
       "      <td>Q4</td>\n",
       "      <td>Standard Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544072</th>\n",
       "      <td>b0271501-03a4-4e0e-b6df-aacf34936878</td>\n",
       "      <td>Blue Cross/Blue Shield of California</td>\n",
       "      <td>False</td>\n",
       "      <td>Blue Cross/Blue Shield of California</td>\n",
       "      <td>Blue Cross/Blue Shield</td>\n",
       "      <td>110000.0</td>\n",
       "      <td>F3200</td>\n",
       "      <td>s</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2022</td>\n",
       "      <td>Q4</td>\n",
       "      <td>Standard Report</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>785595 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       UniqID  \\\n",
       "0        82c5f661-a637-45ad-a3a6-b5ba18cf8962   \n",
       "1        84ad3a9e-5864-4227-a802-e268fbf37237   \n",
       "2        85b111b1-5d2e-4107-bc24-0921316e29a5   \n",
       "3        87822a14-12de-478c-a34d-010fa503e539   \n",
       "4        87ff989d-9d12-4fef-84ef-ab69cd616894   \n",
       "...                                       ...   \n",
       "1544068  0854dd96-ed91-4219-99d9-b84bf06ee2f8   \n",
       "1544069  086b65ba-0abe-4ed0-a3e9-a66c9ecc96a3   \n",
       "1544070  b01b51ba-d426-4750-ad00-0ea3c5aa2317   \n",
       "1544071  b0235c16-0a34-4e06-be62-f42a3bf52d6a   \n",
       "1544072  b0271501-03a4-4e0e-b6df-aacf34936878   \n",
       "\n",
       "                                   Registrant  IsLobbyingFirm  \\\n",
       "0                 AstraZeneca Pharmaceuticals           False   \n",
       "1                           David L Horne LLC            True   \n",
       "2                  Echelon Government Affairs            True   \n",
       "3         Western Telecommunications Alliance           False   \n",
       "4          Financial Executives International           False   \n",
       "...                                       ...             ...   \n",
       "1544068                      American Express           False   \n",
       "1544069                     BL Partners Group            True   \n",
       "1544070                             Vectis DC            True   \n",
       "1544071                         Nickles Group            True   \n",
       "1544072  Blue Cross/Blue Shield of California           False   \n",
       "\n",
       "                                       Client  \\\n",
       "0                 AstraZeneca Pharmaceuticals   \n",
       "1                 Multifamily Lenders Council   \n",
       "2                                Albers Group   \n",
       "3         Western Telecommunications Alliance   \n",
       "4          Financial Executives International   \n",
       "...                                       ...   \n",
       "1544068                      American Express   \n",
       "1544069                                  CTIA   \n",
       "1544070                City of Montebello, CA   \n",
       "1544071                           Walmart Inc   \n",
       "1544072  Blue Cross/Blue Shield of California   \n",
       "\n",
       "                                      UItorg     Amount Catcode Self  \\\n",
       "0                            AstraZeneca PLC  1370000.0   H4300    x   \n",
       "1                Multifamily Lenders Council    15000.0   F4600    n   \n",
       "2                               Albers Group    10000.0   Y4000    n   \n",
       "3        Western Telecommunications Alliance    75000.0   C4000    p   \n",
       "4         Financial Executives International    21650.0   F5000    p   \n",
       "...                                      ...        ...     ...  ...   \n",
       "1544068                     American Express   440000.0   F1400    p   \n",
       "1544069                                 CTIA    30000.0   C4300    i   \n",
       "1544070               City of Montebello, CA    20000.0   X3000    n   \n",
       "1544071                          Walmart Inc    80000.0   G4300    i   \n",
       "1544072               Blue Cross/Blue Shield   110000.0   F3200    s   \n",
       "\n",
       "         IncludeNSFS    Ind  Year Quarter       ReportType  \n",
       "0              False   True  2021      Q4        Amendment  \n",
       "1              False   True  2021      Q4  Standard Report  \n",
       "2              False   True  2021      Q4  Standard Report  \n",
       "3              False   True  2021      Q4  Standard Report  \n",
       "4              False   True  2021      Q4  Standard Report  \n",
       "...              ...    ...   ...     ...              ...  \n",
       "1544068        False   True  2022      Q4  Standard Report  \n",
       "1544069        False  False  2022      Q4  Standard Report  \n",
       "1544070        False   True  2022      Q4  Standard Report  \n",
       "1544071        False  False  2022      Q4  Standard Report  \n",
       "1544072        False   True  2022      Q4  Standard Report  \n",
       "\n",
       "[785595 rows x 13 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract quarter dynamically (first character + \"Q\")\n",
    "df_lobbying['Quarter'] = 'Q' + df_lobbying['Type'].str.extract(r'(\\d)')[0]\n",
    "\n",
    "# Extract report type based on suffix in 'Type' column\n",
    "df_lobbying['ReportType'] = df_lobbying['Type'].apply(lambda x: \n",
    "    'Standard Report' if len(x) == 2 else\n",
    "    'Amendment' if x.endswith(\"a\") else\n",
    "    'Amendment (No Activity)' if x.endswith('an') else\n",
    "    'No Activity' if x.endswith('n') else\n",
    "    'Termination' if x.endswith('t') else\n",
    "    \"Termination Amendment\" if x.endswith('ta') else\n",
    "    'Termination (No Activity)' if x.endswith('tn') else\n",
    "    'Unknown')\n",
    "\n",
    "df_lobbying.drop('Type',axis=1) # Dropping 'Type' column as it is uneccessary now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?export=download&id=1yldZYLcZTLIMQ5Xa-e-ecGgQGFWv5n4A\n",
      "To: /Users/kristianmadslangrud/Documents/GitHub/master_thesis25/lob_indus.csv\n",
      "100%|██████████| 20.3M/20.3M [00:00<00:00, 36.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "df_industry = load_csv(lob_indus, save_file = False)\n",
    "df_industry.columns = ['Client','Sub','Total','Year','Catcode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only including records after 2011.\n",
    "df_industry = df_industry[df_industry['Year'] >= 2011]\n",
    "\n",
    "# Capitalizing all inputs in Catcode column\n",
    "df_industry['Catcode'] = df_industry['Catcode'].str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Catcode References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?export=download&id=1vfVA-v-lUMuykF9efl2oDN5DJK2t1PwA\n",
      "To: /Users/kristianmadslangrud/Documents/GitHub/master_thesis25/CatCodeReference.csv\n",
      "100%|██████████| 43.0k/43.0k [00:00<00:00, 1.64MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Try reading the file with extra handling for irregularities\n",
    "df_catcode_references = load_csv(CatCodeReference, save_file = False)\n",
    "df_catcode_references.columns = ['Catcode','Catname','Catorder','Industry','Sector','SectorLong']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging catcode reference with industry column\n",
    "df_industry = pd.merge(df_industry, df_catcode_references[['Catcode','Catname', 'Industry', 'Sector']],\n",
    "                     left_on='Catcode', right_on='Catcode', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?export=download&id=1CFi0Itpi_qg4X4wicg1FIg8S_FJfvCN0\n",
      "From (redirected): https://drive.google.com/uc?export=download&id=1CFi0Itpi_qg4X4wicg1FIg8S_FJfvCN0&confirm=t&uuid=d3c39f91-0094-4207-a767-7c4245f49c30\n",
      "To: /Users/kristianmadslangrud/Documents/GitHub/master_thesis25/lob_agency.csv\n",
      "100%|██████████| 256M/256M [00:06<00:00, 39.3MB/s] \n"
     ]
    }
   ],
   "source": [
    "df_agency = load_csv(lob_agency, save_file=False)\n",
    "df_agency.columns = ['UniqID','AgencyID','Agency']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?export=download&id=18-D_vW4dAHJlvFs4ARK4lgx6KdsAXWoq\n",
      "From (redirected): https://drive.google.com/uc?export=download&id=18-D_vW4dAHJlvFs4ARK4lgx6KdsAXWoq&confirm=t&uuid=1649b749-d0ba-46dc-81d8-d06fbbb35b18\n",
      "To: /Users/kristianmadslangrud/Documents/GitHub/master_thesis25/lob_issue.csv\n",
      "100%|██████████| 787M/787M [00:18<00:00, 43.1MB/s] \n"
     ]
    }
   ],
   "source": [
    "df_issue = load_csv(lob_issue, save_file=False)\n",
    "df_issue.columns = ['SI_ID','UniqID','IssueID','Issue','SpecificIssue','Year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SI_ID</th>\n",
       "      <th>UniqID</th>\n",
       "      <th>Issue</th>\n",
       "      <th>SpecificIssue</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3001624</td>\n",
       "      <td>02e92bd6-0159-495e-9d00-8a490a0be8be</td>\n",
       "      <td>Disaster &amp; Emergency Planning</td>\n",
       "      <td>Issues affecting manufacturer of railroad and ...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3001625</td>\n",
       "      <td>02e92bd6-0159-495e-9d00-8a490a0be8be</td>\n",
       "      <td>Environment &amp; Superfund</td>\n",
       "      <td>Issues affecting manufacturer of railroad and ...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3001626</td>\n",
       "      <td>02e92bd6-0159-495e-9d00-8a490a0be8be</td>\n",
       "      <td>Labor, Antitrust &amp; Workplace</td>\n",
       "      <td>Issues affecting manufacturer of railroad and ...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3001627</td>\n",
       "      <td>02e92bd6-0159-495e-9d00-8a490a0be8be</td>\n",
       "      <td>Roads &amp; Highways</td>\n",
       "      <td>Issues affecting manufacturer of railroad and ...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3001628</td>\n",
       "      <td>02e92bd6-0159-495e-9d00-8a490a0be8be</td>\n",
       "      <td>Railroads</td>\n",
       "      <td>Issues affecting manufacturer of railroad and ...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3009859</th>\n",
       "      <td>3076640</td>\n",
       "      <td>0a62db56-bdc4-4f4c-9329-5b420066f53c</td>\n",
       "      <td>Small Business</td>\n",
       "      <td>Women's Business Center program reauthorizatio...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3009860</th>\n",
       "      <td>3076641</td>\n",
       "      <td>0a6de630-f472-4dc2-a796-b58d67831573</td>\n",
       "      <td>Defense</td>\n",
       "      <td>Issues related to tungsten manufacturing</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3009861</th>\n",
       "      <td>3076642</td>\n",
       "      <td>0a7f2d03-7d7a-450c-b20a-a48c0fa93e92</td>\n",
       "      <td>Automotive Industry</td>\n",
       "      <td>Issues related to autonomous vehicles Cybersec...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3009862</th>\n",
       "      <td>3076643</td>\n",
       "      <td>0a7f2d03-7d7a-450c-b20a-a48c0fa93e92</td>\n",
       "      <td>Banking</td>\n",
       "      <td>Financial services issues Industrial loan bank...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3009863</th>\n",
       "      <td>3076644</td>\n",
       "      <td>0a7f2d03-7d7a-450c-b20a-a48c0fa93e92</td>\n",
       "      <td>Radio &amp; TV Broadcasting</td>\n",
       "      <td>Issues related to connected vehicles 5.9 GHz s...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1873079 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           SI_ID                                UniqID  \\\n",
       "0        3001624  02e92bd6-0159-495e-9d00-8a490a0be8be   \n",
       "1        3001625  02e92bd6-0159-495e-9d00-8a490a0be8be   \n",
       "2        3001626  02e92bd6-0159-495e-9d00-8a490a0be8be   \n",
       "3        3001627  02e92bd6-0159-495e-9d00-8a490a0be8be   \n",
       "4        3001628  02e92bd6-0159-495e-9d00-8a490a0be8be   \n",
       "...          ...                                   ...   \n",
       "3009859  3076640  0a62db56-bdc4-4f4c-9329-5b420066f53c   \n",
       "3009860  3076641  0a6de630-f472-4dc2-a796-b58d67831573   \n",
       "3009861  3076642  0a7f2d03-7d7a-450c-b20a-a48c0fa93e92   \n",
       "3009862  3076643  0a7f2d03-7d7a-450c-b20a-a48c0fa93e92   \n",
       "3009863  3076644  0a7f2d03-7d7a-450c-b20a-a48c0fa93e92   \n",
       "\n",
       "                                 Issue  \\\n",
       "0        Disaster & Emergency Planning   \n",
       "1              Environment & Superfund   \n",
       "2         Labor, Antitrust & Workplace   \n",
       "3                     Roads & Highways   \n",
       "4                            Railroads   \n",
       "...                                ...   \n",
       "3009859                 Small Business   \n",
       "3009860                        Defense   \n",
       "3009861            Automotive Industry   \n",
       "3009862                        Banking   \n",
       "3009863        Radio & TV Broadcasting   \n",
       "\n",
       "                                             SpecificIssue  Year  \n",
       "0        Issues affecting manufacturer of railroad and ...  2022  \n",
       "1        Issues affecting manufacturer of railroad and ...  2022  \n",
       "2        Issues affecting manufacturer of railroad and ...  2022  \n",
       "3        Issues affecting manufacturer of railroad and ...  2022  \n",
       "4        Issues affecting manufacturer of railroad and ...  2022  \n",
       "...                                                    ...   ...  \n",
       "3009859  Women's Business Center program reauthorizatio...  2022  \n",
       "3009860           Issues related to tungsten manufacturing  2022  \n",
       "3009861  Issues related to autonomous vehicles Cybersec...  2022  \n",
       "3009862  Financial services issues Industrial loan bank...  2022  \n",
       "3009863  Issues related to connected vehicles 5.9 GHz s...  2022  \n",
       "\n",
       "[1873079 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only including records after 2011.\n",
    "df_issue = df_issue[df_issue['Year'] >= 2011]\n",
    "\n",
    "# Retrieving records with missing values for SpecificIssue \n",
    "specific_issue_missing = df_issue[df_issue['SpecificIssue'].isna()]\n",
    "\n",
    "# Imputing missing values in SpecificIssue column with value from Issue column. \n",
    "df_issue['SpecificIssue'] = df_issue['SpecificIssue'].fillna(df_issue['Issue'])\n",
    "\n",
    "# Dropping IssueID column as this has no value for our analysis. \n",
    "df_issue.drop('IssueID',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?export=download&id=1qqQvNXtdhyI8KYcctpVUZ3FoIwvjWaG1\n",
      "From (redirected): https://drive.google.com/uc?export=download&id=1qqQvNXtdhyI8KYcctpVUZ3FoIwvjWaG1&confirm=t&uuid=76347a04-920c-4d1c-adbb-e5f6d0aefe85\n",
      "To: /Users/kristianmadslangrud/Documents/GitHub/master_thesis25/lob_bills.csv\n",
      "100%|██████████| 219M/219M [00:05<00:00, 43.7MB/s] \n"
     ]
    }
   ],
   "source": [
    "df_bills = load_csv(lob_bills, save_file=False)\n",
    "df_bills.columns = ['B_ID', 'SI_ID', 'CongNo', 'Bill_Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Year column from the issues dataframe \n",
    "# this is to impute missing values for congress number by cross-referencing Year with a dictionary containing Year and Congress No. at the time. \n",
    "df_bills = df_bills.merge(df_issue[['SI_ID', 'Year']], on='SI_ID', how='left')\n",
    "\n",
    "# Dictionary mapping Congress by year. Retrieved from: https://www.congress.gov/help/field-values/congresses\n",
    "congress_number_by_year = {\n",
    "2025: 119.0, 2026: 119.0,2023: 118.0,2024: 118.0,2021: 117.0,2022: 117.0,2019: 116.0,2020: 116.0,\n",
    "2017: 115.0,2018: 115.0,2015: 114.0,2016: 114.0,2013: 113.0,2014: 113.0,2011: 112.0,\n",
    "2012: 112.0,2009: 111.0,2010: 111.0,2007: 110.0,2008: 110.0,2005: 109.0,2006: 109.0,\n",
    "2003: 108.0,2004: 108.0,2001: 107.0,2002: 107.0,1999: 106.0,2000: 106.0,1998: 105.0\n",
    "}\n",
    "\n",
    "# Impute missing values\n",
    "df_bills['CongNo'] = df_bills['CongNo'].fillna(df_bills['Year'].map(congress_number_by_year))\n",
    "\n",
    "# Dropping 'Year' column again\n",
    "df_bills = df_bills.drop('Year',axis=1)\n",
    "\n",
    "# Converting datatype of CongNo column\n",
    "df_bills['CongNo'] = df_bills['CongNo'].astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping records with missing value in CongNo, as these records are not connected to Lobby Issues dataframe through Foreign Key - and are hence excessive\n",
    "df_bills = df_bills.dropna(subset=['CongNo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bills[\"Bill_Name\"] = df_bills[\"Bill_Name\"].str.replace(r\"^H\\.?(\\d+)$\", r\"H.R.\\1\", regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lobbyist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?export=download&id=1q1ZyLvUhsxsbPLdi_CmrNH0FqLoorr6p\n",
      "From (redirected): https://drive.google.com/uc?export=download&id=1q1ZyLvUhsxsbPLdi_CmrNH0FqLoorr6p&confirm=t&uuid=6bd1b845-62fa-4d10-82b3-c4c47c9698b7\n",
      "To: /Users/kristianmadslangrud/Documents/GitHub/master_thesis25/lob_lobbyist.csv\n",
      "100%|██████████| 410M/410M [00:09<00:00, 42.9MB/s] \n"
     ]
    }
   ],
   "source": [
    "df_lobbyist = load_csv(lob_lobbyist,save_file=False)\n",
    "df_lobbyist.columns = ['UniqID','Lobbyist_raw','Lobbyist','LobbyistID','Year','OfficialPosition','CID','Formercongmem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only including records after 2011.\n",
    "df_lobbyist = df_lobbyist[df_lobbyist['Year'] >= 2011]\n",
    "\n",
    "# Dropping missing values\n",
    "df_lobbyist = df_lobbyist.dropna(subset=['LobbyistID'])\n",
    "df_lobbyist = df_lobbyist.dropna(subset=['Year']) # Dropping this record as well as it does not have any UniqId\n",
    "\n",
    "# Dropping uneccessary column\n",
    "df_lobbyist = df_lobbyist.drop('Lobbyist_raw',axis=1)\n",
    "\n",
    "# Chaging data type of Year column\n",
    "df_lobbyist['Year'] = df_lobbyist['Year'].astype('Int64')\n",
    "\n",
    "# Imputing missing values in 'Formercongmem' column -- if CID = Blank --> 'n', if CID != Blank --> 'y'\n",
    "df_lobbyist.loc[df_lobbyist['Formercongmem'].isna(), 'Formercongmem'] = df_lobbyist['CID'].apply(\n",
    "    lambda x: 'n' if pd.isna(x) else 'y')\n",
    "\n",
    "# Changing data type of Formercongmem column to Boolean\n",
    "df_lobbyist['Formercongmem'] = df_lobbyist['Formercongmem'].map({'y': True, 'n': False}).astype(bool)\n",
    "\n",
    "# Imputing missing values for CID column\n",
    "df_lobbyist['CID'] = df_lobbyist['CID'].fillna(value='Not Congress Member')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41623\n"
     ]
    }
   ],
   "source": [
    "# Pringint unique values in official position column\n",
    "print(len(df_lobbyist['OfficialPosition'].unique()))\n",
    "\n",
    "# Considering the amount of missing values and unique values - OfficialPosition column is dropped due to providing small value to further analysis\n",
    "df_lobbyist = df_lobbyist.drop('OfficialPosition',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Candidate IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?export=download&id=1k2lhAcuSQbn5z7NN3USPEbX_Rg5VV_Ky\n",
      "To: /Users/kristianmadslangrud/Documents/GitHub/master_thesis25/CandidateIds.csv\n",
      "100%|██████████| 970k/970k [00:00<00:00, 8.03MB/s]\n"
     ]
    }
   ],
   "source": [
    "df_candidate = load_csv(CandidateIds,save_file=False)\n",
    "df_candidate.columns = ['CID', 'CRPName', 'Party', 'DistIDRunFor', 'FECCandID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on the common 'CID' column to include the columns from df_candidate\n",
    "df_lobbyist = pd.merge(\n",
    "    df_lobbyist,\n",
    "    df_candidate[['CID', 'CRPName', 'Party', 'DistIDRunFor', 'FECCandID']],\n",
    "    on='CID',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narrowing Scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only including lobbying reports related to specific issues\n",
    "chosen_issues = ['Health Issues', 'Medical Research & Clin Labs', 'Pharmacy', 'Medicare & Medicaid', 'Science & Technology', 'Taxes']\n",
    "df_issue = df_issue[df_issue['Issue'].isin(chosen_issues)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping lobbying reports that are no longer referenced in df_issue table\n",
    "df_lobbying = df_lobbying[df_lobbying['UniqID'].isin(df_issue['UniqID'])]\n",
    "\n",
    "# dropping lobbyist records for lobbyists that are no longer involved in the lobbying reports from cleaned df_lobbying df.\n",
    "df_lobbyist = df_lobbyist[df_lobbyist['UniqID'].isin(df_lobbying['UniqID'])]\n",
    "\n",
    "# droppping bills records that are not related to any of the chosen issues. \n",
    "df_bills = df_bills[df_bills['SI_ID'].isin(df_issue['SI_ID'])]\n",
    "\n",
    "# droppping agency records that are not related to any of the current reports. \n",
    "df_agency = df_agency[df_agency['UniqID'].isin(df_lobbying['UniqID'])]\n",
    "\n",
    "# droppping industry records that are not related to any of the current reports.\n",
    "df_industry = df_industry[df_industry['Catcode'].isin(df_lobbying['Catcode'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing additional information about bills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?export=download&id=1-MnzV0iZ2FPYiuQgOmggjmXvxGmBzYV8\n",
      "To: /Users/kristianmadslangrud/Documents/GitHub/master_thesis25/congress_bill_data.csv\n",
      "100%|██████████| 4.26M/4.26M [00:00<00:00, 18.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "df_congress_bill_data = load_csv(congress_bill_data_id, save_file=False)\n",
    "df_congress_bill_data.columns = ['bill_number','congress_number','bill_title','policy_area','introduced_date',\n",
    "                                 'bill_type','full_bill_code','sponsor_name','sponsor_party','sponsor_bioguide_id']\n",
    "df_congress_bill_data = df_congress_bill_data.iloc[1:] # dropping first row of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_congress_bill_data['congress_number'] = df_congress_bill_data['congress_number'].astype('Int64')\n",
    "\n",
    "df_congress_bill_data = df_congress_bill_data.merge(\n",
    "    df_bills[['B_ID', 'CongNo', 'Bill_Name']],\n",
    "    left_on=['full_bill_code', 'congress_number'],\n",
    "    right_on=['Bill_Name', 'CongNo'],\n",
    "    how='left'\n",
    ").drop(columns=['Bill_Name', 'CongNo'],axis = 1)  # Drop duplicate columns after merge if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping bill records we don't have any data sponsor data on. \n",
    "df_bills = df_bills[df_bills['B_ID'].isin(df_congress_bill_data['B_ID'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restructuring Data for Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all tables on 'UniqID' to capture all required fields\n",
    "df_master_table = (\n",
    "    df_lobbying\n",
    "    .merge(df_agency, on='UniqID', how='left')\n",
    "    .merge(df_lobbyist, on='UniqID', how='left')\n",
    "    .merge(df_issue, on='UniqID', how='left')\n",
    ")\n",
    "\n",
    "# Select only the required unique columns\n",
    "df_master_table = df_master_table[['UniqID', 'AgencyID', 'LobbyistID', 'SI_ID']].drop_duplicates()\n",
    "\n",
    "# Add a unique MasterID\n",
    "df_master_table.insert(0, 'MasterID', range(1001, 1001 + len(df_master_table)))\n",
    "\n",
    "df_master_table['AgencyID'] = df_master_table['AgencyID'].astype('Int64') # changing data type of AgencyID column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# we observe that some lobbying instances are not related to a Lobbyist or Agency. \n",
    "df_master_table.isnull().sum()\n",
    "df_master_table[['AgencyID','LobbyistID']] = df_master_table[['AgencyID','LobbyistID']].replace({np.nan: None}) # Replace NaN with None for SQL compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include year column in master table\n",
    "df_master_table = pd.merge(\n",
    "    df_master_table,\n",
    "    df_lobbying[['UniqID','Year']],\n",
    "    on='UniqID',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Industry Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all unique combinations of Catcode-Year from the Lobbying dataframe are captured\n",
    "unique_catcode_years = df_lobbying[[\"Catcode\", \"Year\"]].drop_duplicates()\n",
    "\n",
    "# Summarize lobbying data (only including Ind=True records)\n",
    "lobbying_filtered_summary = (\n",
    "    df_lobbying[df_lobbying[\"Ind\"] == True]  # Only include records where Ind=True\n",
    "    .groupby([\"Catcode\", \"Year\"])\n",
    "    .agg(\n",
    "        Total_Lobbying_Reported=(\"Amount\", \"sum\"),  # Sum lobbying spending that counts towards industry totals\n",
    "    )\n",
    "    .reset_index())\n",
    "\n",
    "# Summarize industry data by Catcode + Year\n",
    "industry_summary = (\n",
    "    df_industry.groupby([\"Catcode\", \"Year\"])\n",
    "    .agg(\n",
    "        Total_Lobbying_Spending=(\"Total\", \"sum\"),  # Sum total spending per industry per year\n",
    "        Unique_Clients=(\"Client\", \"nunique\")  # Count unique clients per industry per year\n",
    "    )\n",
    "    .reset_index())\n",
    "\n",
    "# Ensure all unique Catcode-Year combinations from Lobbying Data exist in the final dataset\n",
    "df_industry_summary = unique_catcode_years.merge(\n",
    "    lobbying_filtered_summary, on=[\"Catcode\", \"Year\"], how=\"left\").merge(\n",
    "    industry_summary, on=[\"Catcode\", \"Year\"], how=\"left\"\n",
    "    )\n",
    "\n",
    "# Fill NaN values with 0 for numerical columns (in case no data exists for that Catcode-Year in a dataset)\n",
    "df_industry_summary.fillna({\"Total_Lobbying_Reported\": 0, \"Total_Lobbying_Spending\": 0, \"Unique_Clients\": 0}, inplace=True)\n",
    "\n",
    "# Extract unique Catname, Industry, and Sector information per Catcode\n",
    "df_industry_unique = df_industry[[\"Catcode\", \"Catname\", \"Industry\", \"Sector\"]].drop_duplicates()\n",
    "\n",
    "# Merge additional industry metadata\n",
    "df_industry_summary = df_industry_summary.merge(\n",
    "    df_industry_unique,\n",
    "    on=\"Catcode\",\n",
    "    how=\"left\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agency Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicate entries of AgencyID and Agency combination\n",
    "df_agency_final = df_agency[['AgencyID', 'Agency']].drop_duplicates()\n",
    "\n",
    "# Displaying potential duplicates in AgencyID column\n",
    "df_agency_final[df_agency_final.duplicated(subset=['AgencyID'], keep=False)].sort_values('AgencyID')\n",
    "\n",
    "# Define the list of (Agency, AgencyID) tuples to drop\n",
    "to_drop = [\n",
    "    (54,'Federal Deposit Insurance Commission'),\n",
    "    (86, 'Postal Rate Commission'),\n",
    "    (124, 'Defense Security Assistance Agency'),\n",
    "    (199,'ACCESS Board'),\n",
    "    (238, 'Office of National Aids Policy')\n",
    "]\n",
    "\n",
    "# Drop rows that match both Agency and AgencyID\n",
    "df_agency_final = df_agency_final[~df_agency_final.apply(tuple, axis=1).isin(to_drop)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lobbyist Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lobbyist_final = df_lobbyist[['LobbyistID', 'Lobbyist', 'Formercongmem', 'Party']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing values for political party for former congress-men\n",
    "missing_political_party = {\n",
    "    'Y0000022381L' : 'R','Y0000014172L' : 'R','Y0000041393L' : 'R','Y0000041198L' : 'D','Y0000031994L' : 'R','Y0000040464L' : 'R',\n",
    "    'Y0000041214L' : 'D','Y0000040343L' : 'R','Y0000026610L' : 'R','Y0000042045L' : 'R','Y0000041638L' : 'R','Y0000027785L' : 'R',\n",
    "    'Y0000041137L' : 'D','Y0000009728L' : 'R','Y0000041060L' : 'R','Y0000027979L' : 'R','Y0000040107L' : 'D','Y0000030165L' : 'D',\n",
    "    'Y0000008316L' : 'R','Y0000041693L' : 'D','Y0000041437L' : 'D','Y0000000158L' : 'R','Y0000040464M' : 'R','Y0000040575L' : 'R',\n",
    "    'Y0000042027L' : 'D','Y0000009666L' : 'D','Y0000038683L' : 'R','Y0000046534L' : 'R','Y0000041860L' : 'D','Y0000028461L' : 'R',\n",
    "    'Y0000014475L' : 'D','Y0000017606L' : 'D','Y0000011321L' : 'R','Y0000042041L' : 'R','Y0000041375L' : 'D','Y0000041586L' : 'D',\n",
    "    'Y0000041364L' : 'D','Y0000017415L' : 'R','Y0000008436L' : 'R','Y0000045198L' : 'R','Y0000006265L' : 'R','Y0000035408L' : 'R',\n",
    "    'Y0000040391L' : 'R','Y0000041827L' : 'R','Y0000041317L' : 'R','Y0000042035L' : 'D','Y0000040953L' : 'D','Y0000012587L' : 'D',\n",
    "    'Y0000000545L' : 'D','Y0000005168L' : 'D','Y0000005429L' : 'R','Y0000028048L' : 'R','Y0000039653L' : 'D','Y0000015675L' : 'D',\n",
    "    'Y0000014080L' : 'D','Y0000041094L' : 'D','Y0000025402L' : 'D','Y0000033912L' : 'R','Y0000027950L' : 'R','Y0000003201L' : 'D',\n",
    "    'Y0000046532L' : 'D','Y0000027397L' : 'D','Y0000040428L' : 'R','Y0000017752L' : 'D','Y0000000969L' : 'D','Y0000016284L' : 'R',\n",
    "    'Y0000001540L' : 'D','Y0000008788L' : 'D','Y0000055101L' : 'D','Y0000019350L' : 'R','Y0000028862L' : 'D','Y0000047746L' : 'D',\n",
    "    'Y0000020586L' : 'D','Y0000007555L' : 'R'\n",
    "    }\n",
    "\n",
    "df_lobbyist_final['Party'] = df_lobbyist_final['Party'].fillna(df_lobbyist_final['LobbyistID'].map(missing_political_party))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pg/ghgx2xgn3qb65km4qhtfgzhr0000gn/T/ipykernel_58531/3952088457.py:14: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_lobbyist_final = df_lobbyist_final.groupby('LobbyistID', group_keys=False).apply(resolve_duplicates).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# Define a function to handle duplicates based on 'Party' column\n",
    "def resolve_duplicates(group):\n",
    "    # If there's only one entry, return it\n",
    "    if len(group) == 1:\n",
    "        return group\n",
    "\n",
    "    # Check if the first occurrence has a missing value in 'Party'\n",
    "    if pd.isna(group.iloc[0]['Party']):\n",
    "        return group.iloc[1:]  # Keep the second entry\n",
    "    else:\n",
    "        return group.iloc[:1]  # Keep the first entry\n",
    "\n",
    "# Apply the function to grouped data\n",
    "df_lobbyist_final = df_lobbyist_final.groupby('LobbyistID', group_keys=False).apply(resolve_duplicates).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Issue Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keeping necessary columns for new table\n",
    "df_issue_final = df_issue[['SI_ID','Issue','SpecificIssue']]\n",
    "\n",
    "df_issue_final = df_issue_final.merge(\n",
    "    df_bills[['SI_ID','B_ID']],\n",
    "    on = 'SI_ID',\n",
    "    how = 'left'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bills Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bills_final = df_bills[['B_ID','CongNo','Bill_Name']] # only keeping necessary columns\n",
    "df_bills_final = df_bills_final[['B_ID', 'CongNo', 'Bill_Name']].drop_duplicates() # Dropping duplicates from dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# checking if all B_ID's from parent table exists in child table. \n",
    "missing_b_ids_in_bills = df_issue_final[~df_issue_final[\"B_ID\"].isin(df_bills_final[\"B_ID\"])]\n",
    "print(missing_b_ids_in_bills['B_ID'].unique()) # all missing ID's from parent table are missing values. \n",
    "\n",
    "# checking if all B_ID's in child table table exists in parent table. \n",
    "all_b_ids_exist_in_issue = df_bills_final[\"B_ID\"].isin(df_issue_final[\"B_ID\"]).all()\n",
    "print(all_b_ids_exist_in_issue)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sponsor Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pg/ghgx2xgn3qb65km4qhtfgzhr0000gn/T/ipykernel_58531/658583622.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sponsor.rename(columns={'sponsor_bioguide_id':'SponsorID','sponsor_name':'SponsorName','sponsor_party':'SponsorParty'},inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df_sponsor = df_congress_bill_data[['sponsor_bioguide_id','B_ID','sponsor_name','sponsor_party']]\n",
    "\n",
    "df_sponsor.rename(columns={'sponsor_bioguide_id':'SponsorID','sponsor_name':'SponsorName','sponsor_party':'SponsorParty'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# checking if all B_ID's from parent table exists in child table. \n",
    "missing_b_ids_in_sponsor = df_bills_final[~df_bills_final[\"B_ID\"].isin(df_sponsor[\"B_ID\"])]\n",
    "print(missing_b_ids_in_sponsor['B_ID'].unique()) # all missing ID's from parent table are missing values. \n",
    "\n",
    "# checking if all B_ID's in child table table exists in parent table. \n",
    "all_b_ids_exist_in_bills = df_sponsor[\"B_ID\"].isin(df_bills_final[\"B_ID\"]).all()\n",
    "print(all_b_ids_exist_in_bills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SponsorID       95\n",
       "B_ID             0\n",
       "SponsorName     95\n",
       "SponsorParty    95\n",
       "dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sponsor.isnull().sum() # we observe that 95 bills are missing. these are either Reserved for Speaker, or dosen't actually have a Sponsor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sponsor = df_sponsor[['SponsorID','B_ID','SponsorName','SponsorParty']].drop_duplicates() # dropping duplicate records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SponsorID</th>\n",
       "      <th>B_ID</th>\n",
       "      <th>SponsorName</th>\n",
       "      <th>SponsorParty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [SponsorID, B_ID, SponsorName, SponsorParty]\n",
       "Index: []"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if one bill can be related to multiple sponsors\n",
    "duplicated_bills = df_sponsor[df_sponsor.duplicated(subset=['B_ID'], keep=False)]\n",
    "duplicated_bills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# considering we have a one-to-many relationship between sponsor and bills, we can simply merge the two tables by including sponsor information in the Bills Table. \n",
    "df_bills_final = df_bills_final.merge(df_sponsor[['B_ID','SponsorName','SponsorParty']], on = 'B_ID', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Industry and Lobbying Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create foreign key for df_industry_summary table my concatenating catcode and year in df_lobbying\n",
    "df_lobbying['industry_id'] = df_lobbying['Catcode'].astype(str) + \"_\" + df_lobbying['Year'].astype(str)\n",
    "\n",
    "# dropping year and catcode column from df_lobbying\n",
    "df_lobbying = df_lobbying.drop(columns=['Catcode','Year'],axis = 1)\n",
    "\n",
    "# creating industry_id and dropping year column for df_industry_summary\n",
    "df_industry_summary['industry_id'] = df_industry_summary['Catcode'].astype(str) + \"_\" + df_industry_summary['Year'].astype(str)\n",
    "df_industry_summary = df_industry_summary.drop(columns=['Year'],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Junction Table between Issue and Bills: Issue_Bills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create junction dataframe\n",
    "df_bills_issues = df_issue_final[['SI_ID', 'B_ID']].drop_duplicates()\n",
    "\n",
    "# cleaning issue table to not reference bills\n",
    "df_issue_final = df_issue_final.drop(columns=['B_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bills_issues.isnull().sum() # about 15% of issue records are not related to a bill.  \n",
    "df_bills_issues['B_ID'] = df_bills_issues['B_ID'].replace({np.nan: None}) # Replace NaN with None for SQL compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure we don't have duplicated entries for PKs in df_bills_final and df_issues_final\n",
    "df_bills_final[df_bills_final.duplicated(subset='B_ID', keep=False)] # no duplciates\n",
    "df_issue_final[df_issue_final.duplicated(subset='SI_ID', keep=False)] # duplicats, only keep first occurence of \"SI_ID\"\n",
    "df_issue_final = df_issue_final.drop_duplicates(subset='SI_ID', keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data for Relational Database Management System (RDBMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2 \n",
    "\n",
    "# Database connection parameters\n",
    "host = 'localhost'\n",
    "database = 'USLobbyism2'\n",
    "user = 'postgres'\n",
    "password = 'MasterThesis'\n",
    "port = 5433\n",
    "\n",
    "\n",
    "connection = psycopg2.connect(\n",
    "    host=host,\n",
    "    database=database,\n",
    "    user=user,\n",
    "    password=password,\n",
    "    port=port \n",
    ")\n",
    "\n",
    "cur = connection.cursor()\n",
    "\n",
    "# Create the dimension tables\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS industry_dim (\n",
    "        industry_id VARCHAR(36) PRIMARY KEY,\n",
    "        Catcode CHAR(5),\n",
    "        Catname VARCHAR(50),\n",
    "        Industry VARCHAR(36),\n",
    "        Sector VARCHAR(50),\n",
    "        UniqueClients INT,\n",
    "        Total_Lobbying_Reported FLOAT,\n",
    "        Total_Lobbying_Spending FLOAT\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS lobbying_report_dim (\n",
    "        UniqID VARCHAR(36) PRIMARY KEY,\n",
    "        Registrant VARCHAR(50),\n",
    "        IsLobbyingFirm BOOLEAN,\n",
    "        Client VARCHAR(50),\n",
    "        UItorg VARCHAR(50),\n",
    "        Amount FLOAT,\n",
    "        Self CHAR(1),\n",
    "        IncludeNSFS BOOLEAN,\n",
    "        Ind BOOLEAN,\n",
    "        Quarter CHAR(2),\n",
    "        ReportType VARCHAR(36),\n",
    "        industry_id VARCHAR(36),\n",
    "        FOREIGN KEY (industry_id) REFERENCES industry_dim(industry_id) ON DELETE CASCADE\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS agency_dim (\n",
    "        AgencyID CHAR(4) PRIMARY KEY,\n",
    "        Agency VARCHAR(80)\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS lobbyist_dim (\n",
    "        LobbyistID VARCHAR(12) PRIMARY KEY,\n",
    "        LobbyistName VARCHAR(50),\n",
    "        Formercongmem BOOLEAN,\n",
    "        Party CHAR(4)\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS bills_dim (\n",
    "        B_ID VARCHAR(18) PRIMARY KEY,\n",
    "        BillName CHAR(15),\n",
    "        CongNo CHAR(3),\n",
    "        SponsorName VARCHAR(255),\n",
    "        SponsorParty CHAR(4)\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS issue_dim (\n",
    "        SI_ID BIGINT PRIMARY KEY,\n",
    "        Issue VARCHAR(50),\n",
    "        SpecificIssue TEXT\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS bills_issues_junction (\n",
    "        JunctionID SERIAL PRIMARY KEY,\n",
    "        B_ID VARCHAR(18),\n",
    "        SI_ID BIGINT NOT NULL,\n",
    "        FOREIGN KEY (B_ID) REFERENCES bills_dim(B_ID) ON DELETE SET NULL,\n",
    "        FOREIGN KEY (SI_ID) REFERENCES issue_dim(SI_ID) ON DELETE CASCADE\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS main_fact (\n",
    "        MasterID SERIAL PRIMARY KEY,\n",
    "        UniqID VARCHAR(36) NOT NULL,\n",
    "        LobbyistID VARCHAR(12),\n",
    "        SI_ID BIGINT NOT NULL,\n",
    "        AgencyID CHAR(4),\n",
    "        Year INT,\n",
    "        FOREIGN KEY (UniqID) REFERENCES lobbying_report_dim(UniqID) ON DELETE CASCADE,\n",
    "        FOREIGN KEY (LobbyistID) REFERENCES lobbyist_dim(LobbyistID) ON DELETE SET NULL,\n",
    "        FOREIGN KEY (SI_ID) REFERENCES issue_dim(SI_ID) ON DELETE CASCADE,\n",
    "        FOREIGN KEY (AgencyID) REFERENCES agency_dim(AgencyID) ON DELETE SET NULL\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Insert data into tables\n",
    "\n",
    "cur.executemany(\"\"\"\n",
    "    INSERT INTO industry_dim (industry_id, Catcode, Catname, Industry, Sector, UniqueClients, Total_Lobbying_Reported, Total_Lobbying_Spending)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\", df_industry_summary[['industry_id','Catcode', 'Catname', 'Industry', 'Sector', 'Unique_Clients', 'Total_Lobbying_Reported', 'Total_Lobbying_Spending']].values.tolist())\n",
    "\n",
    "cur.executemany(\"\"\"\n",
    "    INSERT INTO lobbying_report_dim (UniqID, Registrant, IsLobbyingFirm, Client, UItorg, Amount, Self, IncludeNSFS, Ind, Quarter, ReportType, industry_id)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\", df_lobbying[['UniqID','Registrant','IsLobbyingFirm','Client','UItorg','Amount','Self','IncludeNSFS','Ind','Quarter','ReportType','industry_id']].values.tolist())\n",
    "\n",
    "cur.executemany(\"INSERT INTO agency_dim (AgencyID, Agency) VALUES (%s, %s)\", df_agency_final[['AgencyID', 'Agency']].values.tolist())\n",
    "cur.executemany(\"INSERT INTO lobbyist_dim (LobbyistID, LobbyistName, Formercongmem, Party) VALUES (%s, %s, %s, %s)\", df_lobbyist_final[['LobbyistID','Lobbyist', 'Formercongmem', 'Party']].values.tolist())\n",
    "cur.executemany(\"INSERT INTO bills_dim (B_ID, BillName, CongNo, SponsorName, SponsorParty) VALUES (%s, %s, %s, %s, %s)\", df_bills_final[['B_ID','Bill_Name', 'CongNo','SponsorName','SponsorParty']].values.tolist())\n",
    "cur.executemany(\"INSERT INTO issue_dim (SI_ID, Issue, SpecificIssue) VALUES (%s, %s, %s)\", df_issue_final[['SI_ID','Issue', 'SpecificIssue']].values.tolist())\n",
    "cur.executemany(\"INSERT INTO bills_issues_junction (B_ID, SI_ID) VALUES (%s, %s)\", df_bills_issues[['B_ID','SI_ID']].values.tolist())\n",
    "cur.executemany(\"INSERT INTO main_fact (MasterID, UniqID, LobbyistID, SI_ID, AgencyID, Year) VALUES (%s, %s, %s, %s, %s, %s)\", df_master_table[['MasterID','UniqID','LobbyistID','SI_ID', 'AgencyID','Year']].values.tolist())\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "connection.commit()\n",
    "cur.close()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download if necessary\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#spacy.cli.download(\"en_core_web_sm\")\n",
    "#nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific Issue Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kristianmadslangrud/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "100%|██████████| 421309/421309 [41:40<00:00, 168.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        SpecificIssue  \\\n",
      "0   Issues affecting manufacturer of railroad and ...   \n",
      "1   Support for education and research of health i...   \n",
      "3   H.R. 4750, the Performing Artist Tax Parity Ac...   \n",
      "6   Infectious diseases (generally, no specific le...   \n",
      "11  Antimicrobial resistance research, infectious ...   \n",
      "\n",
      "                                   Preprocessed_Issue  \n",
      "0   issue affect manufacturer railroad utility pro...  \n",
      "1   support education research health issue concer...  \n",
      "3   Performing Artist Tax Parity Act PATPA America...  \n",
      "6   infectious disease generally specific legislat...  \n",
      "11  antimicrobial resistance research infectious d...  \n"
     ]
    }
   ],
   "source": [
    "# retrieve specific issue text\n",
    "texts_specific_issue = df_issue_final[\"SpecificIssue\"]\n",
    "\n",
    "# function for preprocessing with tqdm og nlp.pipe\n",
    "def preprocess_texts(texts):\n",
    "    processed = []\n",
    "    for doc in tqdm(nlp.pipe(texts, batch_size=50), total=len(texts)):\n",
    "        tokens = [\n",
    "            token.lemma_.lower() for token in doc\n",
    "            if token.is_alpha and token.lemma_.lower() not in stop_words and len(token.lemma_) > 2\n",
    "        ]\n",
    "        processed.append(\" \".join(tokens))\n",
    "    return processed\n",
    "\n",
    "# apply function\n",
    "df_issue_final[\"PreprocessedIssue\"] = preprocess_texts(texts_specific_issue)\n",
    "\n",
    "# display result\n",
    "print(df_issue_final[[\"SpecificIssue\", \"PreprocessedIssue\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet Processsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = load_csv(tweet_data,gzip=True)\n",
    "df_tweets.columns = ['id','screen_name','user_id','text','date']\n",
    "df_tweets = df_tweets.iloc[1:] # dropping first row of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.columns = ['id','screen_name','user_id','text','date']\n",
    "df_tweets = df_tweets.iloc[1:] # dropping first row of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "screen_name    0\n",
       "user_id        0\n",
       "text           0\n",
       "date           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets = df_tweets.dropna(subset=['text'])\n",
    "df_tweets.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6268526/6268526 [10:24:01<00:00, 167.42it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "2  Big dangerous storm hit my sister's neighborho...   \n",
      "3  @jeffmw Thanks Jeff.  Happy New Year! Here's t...   \n",
      "4  5,4,3,2,1..happy new year! I know I'm early, b...   \n",
      "5            Wishing you and yours a Happy New Year!   \n",
      "6     Wishing everyone a happy and healthy new year.   \n",
      "\n",
      "                                   preprocessed_text  \n",
      "2  big dangerous storm hit sister neighborhood Su...  \n",
      "3  thank Jeff Happy New Year find thing agree nex...  \n",
      "4  happy new year know early time swear twitter p...  \n",
      "5                                wish Happy New Year  \n",
      "6               wish everyone happy healthy new year  \n"
     ]
    }
   ],
   "source": [
    "# retrieve specific issue text\n",
    "texts_tweets = df_tweets[\"text\"]\n",
    "\n",
    "# apply function\n",
    "df_tweets[\"preprocessed_text\"] = preprocess_texts(texts_tweets)\n",
    "\n",
    "# display result\n",
    "print(df_tweets[[\"text\", \"preprocessed_text\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing tweets over time\n",
    "df_tweets['date'] = pd.to_datetime(df_tweets['date'])\n",
    "\n",
    "tweet_counts = df_tweets.groupby('date').size()  # Count tweets per day\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(tweet_counts.index, tweet_counts.values, marker='o', linestyle='-', color='b')\n",
    "\n",
    "plt.title('Number of Tweets per Day')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Tweets')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERTTopic on Issue Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristianmadslangrud/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 420706 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 13148/13148 [08:53<00:00, 24.66it/s]\n",
      "2025-04-09 19:21:16,890 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting BERTopic model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n",
      "2025-04-10 04:01:38,535 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-10 04:01:38,564 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2025-04-10 04:01:57,781 - BERTopic - Cluster - Completed ✓\n",
      "2025-04-10 04:01:57,867 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2025-04-10 04:02:02,708 - BERTopic - Representation - Completed ✓\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# Drop NA and reset index to ensure clean list of documents\n",
    "issue_documents = df_issue_final[\"Preprocessed_Issue\"].dropna().reset_index(drop=True).tolist()\n",
    "\n",
    "# Initialize the sentence transformer embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Track embedding progress\n",
    "print(f\"Generating embeddings for {len(issue_documents)} documents...\")\n",
    "embeddings = embedding_model.encode(issue_documents, show_progress_bar=True, batch_size=32)\n",
    "\n",
    "# Configure HDBSCAN for clustering to limit topic count\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=100,  # minimum number of documents within each topic. \n",
    "    min_samples=5,        \n",
    "    metric='euclidean',   # Distance metric used for clustering\n",
    "    prediction_data=True  \n",
    ")\n",
    "\n",
    "# Initialize BERTopic with the custom HDBSCAN model\n",
    "topic_model = BERTopic(\n",
    "    language=\"english\",\n",
    "    verbose=True,\n",
    "    hdbscan_model=hdbscan_model\n",
    ")\n",
    "\n",
    "# Fit BERTopic model using precomputed embeddings\n",
    "print(\"Fitting BERTopic model...\")\n",
    "topics, probs = topic_model.fit_transform(issue_documents, embeddings)\n",
    "\n",
    "# Save topic assignments back into DataFrame\n",
    "df_issue_final.loc[df_issue_final[\"Preprocessed_Issue\"].notna(), \"BERTopic\"] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_issue_topic_info = topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting information about top words for each topic. \n",
    "bert_issue_topics_dict = topic_model.get_topics()\n",
    "bert_issue_topic_words = []\n",
    "\n",
    "for topic_num, words in bert_issue_topics_dict.items():\n",
    "    bert_issue_topic_words.append({\n",
    "        \"Topic\": topic_num,\n",
    "        \"Words\": \", \".join([word for word, _ in words])\n",
    "    })\n",
    "df_topic_words = pd.DataFrame(bert_issue_topic_words)\n",
    "df_topic_words.to_csv(\"topic_words_less_topics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA on Specific Issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 0.212*\"tax\" + 0.048*\"reform\" + 0.033*\"act\" + 0.025*\"corporate\" + 0.019*\"taxation\" + 0.018*\"business\" + 0.018*\"international\" + 0.016*\"jobs\" + 0.016*\"income\" + 0.013*\"cuts\"\n",
      "Topic 1: 0.064*\"research\" + 0.049*\"funding\" + 0.041*\"appropriations\" + 0.027*\"education\" + 0.026*\"development\" + 0.025*\"national\" + 0.021*\"related\" + 0.021*\"program\" + 0.020*\"science\" + 0.018*\"technology\"\n",
      "Topic 2: 0.025*\"bill\" + 0.021*\"provide\" + 0.017*\"plan\" + 0.014*\"congress\" + 0.012*\"amend\" + 0.011*\"support\" + 0.010*\"retirement\" + 0.010*\"service\" + 0.009*\"individual\" + 0.009*\"purpose\"\n",
      "Topic 3: 0.107*\"support\" + 0.022*\"advocate\" + 0.022*\"cancer\" + 0.017*\"clinical\" + 0.013*\"treatment\" + 0.013*\"laboratory\" + 0.012*\"testing\" + 0.012*\"disease\" + 0.012*\"legislation\" + 0.012*\"use\"\n",
      "Topic 4: 0.280*\"act\" + 0.046*\"provision\" + 0.030*\"american\" + 0.028*\"appropriations\" + 0.027*\"consolidated\" + 0.026*\"relief\" + 0.018*\"better\" + 0.017*\"build\" + 0.017*\"budget\" + 0.016*\"back\"\n",
      "Topic 5: 0.240*\"issue\" + 0.222*\"relate\" + 0.040*\"legislation\" + 0.038*\"policy\" + 0.031*\"include\" + 0.025*\"general\" + 0.022*\"monitor\" + 0.019*\"regard\" + 0.019*\"regulation\" + 0.016*\"legislative\"\n",
      "Topic 6: 0.131*\"act\" + 0.122*\"health\" + 0.065*\"care\" + 0.019*\"public\" + 0.016*\"access\" + 0.015*\"patient\" + 0.014*\"mental\" + 0.014*\"implementation\" + 0.013*\"affordable\" + 0.012*\"protection\"\n",
      "Topic 7: 0.100*\"act\" + 0.095*\"drug\" + 0.029*\"prescription\" + 0.028*\"pricing\" + 0.018*\"medical\" + 0.016*\"access\" + 0.015*\"inflation\" + 0.014*\"cost\" + 0.013*\"fda\" + 0.013*\"reduction\"\n",
      "Topic 8: 0.119*\"medicare\" + 0.048*\"medicaid\" + 0.042*\"payment\" + 0.028*\"hospital\" + 0.024*\"program\" + 0.021*\"reimbursement\" + 0.020*\"physician\" + 0.019*\"access\" + 0.017*\"coverage\" + 0.017*\"part\"\n",
      "Topic 9: 0.104*\"tax\" + 0.087*\"credit\" + 0.047*\"energy\" + 0.021*\"incentive\" + 0.021*\"code\" + 0.021*\"revenue\" + 0.018*\"internal\" + 0.016*\"act\" + 0.014*\"section\" + 0.013*\"clean\"\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "texts = df_issue_final[\"Preprocessed_Issue\"].dropna().tolist()\n",
    "\n",
    "# Split preprocessed text into tokens\n",
    "tokenized_texts = [text.split() for text in texts]\n",
    "\n",
    "# Create a dictionary representation of the documents\n",
    "dictionary = corpora.Dictionary(tokenized_texts)\n",
    "\n",
    "# Filter extremes\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5) # no words that appear in less than 5 documents, or more than 50%\n",
    "\n",
    "# Create a bag-of-words representation for each document\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model = gensim.models.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=10, # 10 unique topics\n",
    "    random_state=42,\n",
    "    passes=10,\n",
    "    alpha='auto',\n",
    "    per_word_topics=True\n",
    ")\n",
    "\n",
    "# Print the topics and their top keywords\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Topic {idx}: {topic}\")\n",
    "\n",
    "# Assign the dominant topic to each document\n",
    "def get_dominant_topic(bow):\n",
    "    topic_probs = lda_model.get_document_topics(bow)\n",
    "    if topic_probs:\n",
    "        return max(topic_probs, key=lambda x: x[1])[0]\n",
    "    return -1\n",
    "\n",
    "# Apply to corpus\n",
    "df_issue_final.loc[df_issue_final[\"Preprocessed_Issue\"].notna(), \"LDATopic\"] = [\n",
    "    get_dominant_topic(bow) for bow in corpus\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract topic keywords into a list of dictionaries\n",
    "topics_list = []\n",
    "for topic_id, topic in lda_model.show_topics(num_topics=-1, num_words=10, formatted=False):\n",
    "    for rank, (word, weight) in enumerate(topic):\n",
    "        topics_list.append({\n",
    "            \"Topic\": topic_id,\n",
    "            \"Rank\": rank + 1,\n",
    "            \"Word\": word,\n",
    "            \"Weight\": weight\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_lda_topics = pd.DataFrame(topics_list)\n",
    "\n",
    "# Save to CSV\n",
    "df_lda_topics.to_csv(\"lda_topics_keywords.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERTopic on Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "df_tweets = pd.read_csv(\"tweets_cleaned.csv\")\n",
    "tweet_texts = df_tweets[\"preprocessed_text\"].fillna(\"\").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12538/12538 [2:24:10<00:00,  1.45it/s] \n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Build topic dictionary\n",
    "top_n = 10\n",
    "topic_keywords = (\n",
    "    df_topic_words.groupby(\"Topic\")[\"Words\"]\n",
    "    .apply(lambda words: \" \".join(words.head(top_n)))\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Load embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Prepare topic embeddings\n",
    "topic_ids = list(topic_keywords.keys())\n",
    "topic_sentences = [topic_keywords[tid] for tid in topic_ids]\n",
    "topic_embeddings = model.encode(topic_sentences, convert_to_tensor=False)\n",
    "\n",
    "# Batched tweet encoding and similarity scoring\n",
    "def batched_topic_match(texts, batch_size=500):\n",
    "    all_matched_topics = []\n",
    "    all_scores = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        batch_embeddings = model.encode(batch, convert_to_tensor=False)\n",
    "        sim_matrix = cosine_similarity(batch_embeddings, topic_embeddings)\n",
    "\n",
    "        best_indices = np.argmax(sim_matrix, axis=1)\n",
    "        best_scores = np.max(sim_matrix, axis=1)\n",
    "        matched_ids = [topic_ids[idx] for idx in best_indices]\n",
    "\n",
    "        all_matched_topics.extend(matched_ids)\n",
    "        all_scores.extend(best_scores)\n",
    "\n",
    "    return all_matched_topics, all_scores\n",
    "\n",
    "# Run the batched matching\n",
    "tweet_texts = df_tweets[\"preprocessed_text\"].fillna(\"\").tolist()\n",
    "df_tweets[\"BERTTopic\"], df_tweets[\"Topic_Similarity\"] = batched_topic_match(tweet_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.to_csv('tweets_with_bert.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA on Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 6268526 tweets in batches of 1000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6269/6269 [07:01<00:00, 14.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize tweets\n",
    "tokenized_tweets = [text.split() for text in tweet_texts]\n",
    "\n",
    "# Convert to BoW using the same dictionary from LDA model\n",
    "tweet_corpus = [dictionary.doc2bow(text) for text in tokenized_tweets]\n",
    "\n",
    "# Function to get dominant topic and score\n",
    "def get_topic_with_score(bow):\n",
    "    topic_probs = lda_model.get_document_topics(bow)\n",
    "    if topic_probs:\n",
    "        dominant_topic, score = max(topic_probs, key=lambda x: x[1])\n",
    "        return dominant_topic, score\n",
    "    return -1, 0.0\n",
    "\n",
    "# Batching setup\n",
    "batch_size = 1000\n",
    "dominant_topics = []\n",
    "topic_scores = []\n",
    "\n",
    "print(f\"Processing {len(tweet_corpus)} tweets in batches of {batch_size}...\")\n",
    "\n",
    "for i in tqdm(range(0, len(tweet_corpus), batch_size)):\n",
    "    batch = tweet_corpus[i:i + batch_size]\n",
    "    for bow in batch:\n",
    "        topic, score = get_topic_with_score(bow)\n",
    "        dominant_topics.append(topic)\n",
    "        topic_scores.append(score)\n",
    "\n",
    "# Store in DataFrame\n",
    "df_tweets[\"LDA_Topic\"] = dominant_topics\n",
    "df_tweets[\"Topic_Score\"] = topic_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis with VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_vader_sentiment(text):\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    return scores['compound']\n",
    "\n",
    "# Apply to your DataFrame\n",
    "df_tweets[\"VADER_Compound\"] = df_tweets[\"preprocessed_text\"].fillna(\"\").apply(get_vader_sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis with twitter-roberta-base-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_tweets = pd.read_csv('tweets_with_topic_modelling_VADER.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(654411, 12)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets = df_tweets[df_tweets['Topic_Similarity']>=0.45] # only keep tweets with over 0.5 topic similarity to one of the topics created from specific issue\n",
    "df_tweets = df_tweets[df_tweets['BERTTopic'].map(df_tweets['BERTTopic'].value_counts()) >= 1000] # only keep topics with more than 100 tweets belonging to them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying RoBERTa sentiment model to 654411 tweets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 4887/10226 [4:47:09<7:23:21,  4.98s/it] "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the tokenizer and model from Hugging Face\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Create sentiment pipeline\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "# Prepare tweet text\n",
    "texts = df_tweets[\"preprocessed_text\"].fillna(\"\").tolist()\n",
    "\n",
    "# Batched inference to avoid memory issues\n",
    "batch_size = 64\n",
    "labels = []\n",
    "scores = []\n",
    "\n",
    "print(f\"Applying RoBERTa sentiment model to {len(texts)} tweets...\")\n",
    "\n",
    "for i in tqdm(range(0, len(texts), batch_size)):\n",
    "    batch = texts[i:i + batch_size]\n",
    "    results = sentiment_pipeline(batch)\n",
    "    for result in results:\n",
    "        labels.append(result[\"label\"])\n",
    "        scores.append(result[\"score\"])\n",
    "\n",
    "# Add results to DataFrame\n",
    "df_tweets[\"RoBERTa_Label\"] = labels\n",
    "df_tweets[\"RoBERTa_Score\"] = scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.to_csv('final_tweet_analysis.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
